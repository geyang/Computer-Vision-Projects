{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ruchaa_50230340",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix5dQS2rUMlu",
        "colab_type": "text"
      },
      "source": [
        "#EECS 504 PS4: Backpropagation\n",
        "\n",
        "Please provide the following information \n",
        "(e.g. Andrew Owens, ahowens):\n",
        "\n",
        "Rucha Apte, ruchaa\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_Cst4k4tuBc",
        "colab_type": "text"
      },
      "source": [
        "# Starting\n",
        "\n",
        "Run the following code to import the modules you'll need. After your finish the assignment, remember to run all cells and save the note book to your local machine as a .ipynb file for Canvas submission."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHumIO-xt57H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from torchvision.datasets import CIFAR10\n",
        "download = not os.path.isdir('cifar-10-batches-py')\n",
        "dset_train = CIFAR10(root='.', download=download)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apEPzDNtK0MC",
        "colab_type": "text"
      },
      "source": [
        "# Problem 4.2 Multi-layer perceptron\n",
        "In this problem you will develop a two Layer neural network with fully-connected layers to perform classification, and test it out on the CIFAR-10 dataset.\n",
        "\n",
        "We train the network with a softmax loss function on the weight matrices. The network uses a ReLU nonlinearity after the first fully connected layer. In other words, the network has the following architecture:\n",
        "\n",
        "input - fully connected layer - ReLU - fully connected layer - softmax\n",
        "\n",
        "The outputs of the second fully-connected layer are the scores for each class.\n",
        "\n",
        "You cannot use any deep learning libraries such as PyTorch in this part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXfumCQ21JoK",
        "colab_type": "text"
      },
      "source": [
        "# 4.2 (a) Layers\n",
        "In this problem, implement fully connected layer, relu and softmax. Filling in all TODOs in skeleton codes will be sufficient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-ljfgMv9PHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fc_forward(x, w, b):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for a fully-connected layer.\n",
        "    \n",
        "    The input x has shape (N, Din) and contains a minibatch of N\n",
        "    examples, where each example x[i] has shape (Din,).\n",
        "    \n",
        "    Inputs:\n",
        "    - x: A numpy array containing input data, of shape (N, Din)\n",
        "    - w: A numpy array of weights, of shape (Din, Dout)\n",
        "    - b: A numpy array of biases, of shape (Dout,)\n",
        "    \n",
        "    Returns a tuple of:\n",
        "    - out: output, of shape (N, Dout)\n",
        "    - cache: (x, w, b)\n",
        "    \"\"\"\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the forward pass. Store the result in out.              #\n",
        "    ###########################################################################\n",
        "    N = x.shape[0]\n",
        "    #M = x.shape[1:]\n",
        "    M = np.prod(x.shape[1:])\n",
        "    x_new = np.reshape(x,(N,M))\n",
        "    out = np.dot(x_new,w) + b\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    cache = (x, w, b)\n",
        "    return out, cache\n",
        "\n",
        "\n",
        "def fc_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for a fully_connected layer.\n",
        "    \n",
        "    Inputs:\n",
        "    - dout: Upstream derivative, of shape (N, Dout)\n",
        "    - cache: returned by your forward function. Tuple of:\n",
        "      - x: Input data, of shape (N, Din)\n",
        "      - w: Weights, of shape (Din, Dout)\n",
        "      - b: Biases, of shape (Dout,)\n",
        "      \n",
        "    Returns a tuple of:\n",
        "    - dx: Gradient with respect to x, of shape (N, Din)\n",
        "    - dw: Gradient with respect to w, of shape (Din, Dout)\n",
        "    - db: Gradient with respect to b, of shape (Dout,)\n",
        "    \"\"\"\n",
        "    x, w, b = cache\n",
        "    dx, dw, db = None, None, None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the affine backward pass.                               #\n",
        "    ###########################################################################\n",
        "    N = x.shape[0] \n",
        "    #M = x.shape[1:]\n",
        "    M=np.prod(x.shape[1:])\n",
        "    x_new = np.reshape(x,(N,M))\n",
        "\n",
        "    dx_mid = np.dot(dout,np.transpose(w))\n",
        "\n",
        "    dx = np.reshape(dx_mid, x.shape)   \n",
        "    dw = np.dot(np.transpose(x_new), dout)      \n",
        "    db = np.dot(np.transpose(dout), np.ones(N))\n",
        "\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return dx, dw, db\n",
        "\n",
        "def relu_forward(x):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
        "\n",
        "    Input:\n",
        "    - x: Inputs, of any shape\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: Output, of the same shape as x\n",
        "    - cache: x\n",
        "    \"\"\"\n",
        "    out = x\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the ReLU forward pass.                                  #\n",
        "    ###########################################################################\n",
        "    out = np.maximum(x,0)\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    cache = x\n",
        "    return out, cache\n",
        "\n",
        "\n",
        "def relu_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
        "\n",
        "    Input:\n",
        "    - dout: Upstream derivatives, of any shape\n",
        "    - cache: returned by your forward function. Input x, of same shape as dout\n",
        "\n",
        "    Returns:\n",
        "    - dx: Gradient with respect to x\n",
        "    \"\"\"\n",
        "    dx, x = dout, cache\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the ReLU backward pass.                                 #\n",
        "    ###########################################################################\n",
        "    #m,n=dout.shape\n",
        "    #for k in range (m):\n",
        "      #if dx[k] <= 0:\n",
        "        #dx[k] = 0\n",
        "    #dx[x > 0] = 1    \n",
        "    dx[x <= 0] = 0\n",
        "    #dx= 1. * (x > 0) \n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return dx\n",
        "\n",
        "\n",
        "def softmax_loss(x, y):\n",
        "    \"\"\"\n",
        "    Computes the loss and gradient for softmax classification.\n",
        "\n",
        "    Inputs:\n",
        "    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth\n",
        "      class for the ith input.\n",
        "    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
        "      0 <= y[i] < C\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - loss: Scalar giving the loss\n",
        "    - dx: Gradient of the loss with respect to x\n",
        "    \"\"\"\n",
        "    loss, dx = None, None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement softmax loss                                            #\n",
        "    ###########################################################################\n",
        "    softmax = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    #softmax = np.exp(x - np.max(x, axis=1))\n",
        "    softmax = softmax/(np.sum(softmax, axis=1, keepdims=True))\n",
        "    #softmax = softmax/(np.sum(softmax, axis=1))\n",
        "    N,C = x.shape\n",
        "    loss = -np.sum(np.log(softmax[np.arange(N), y])) / N\n",
        "    \n",
        "    softmax[np.arange(N), y] =softmax[np.arange(N), y] - 1\n",
        "    dx = softmax / N\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return loss, dx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbFxtS3zK8oz",
        "colab_type": "text"
      },
      "source": [
        "# 4.2 (b) Softmax Classifier\n",
        "\n",
        "In this problem, implement softmax classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytvxbx9UpxVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SoftmaxClassifier(object):\n",
        "    \"\"\"\n",
        "    A fully-connected neural network with\n",
        "    softmax loss that uses a modular layer design. We assume an input dimension\n",
        "    of D, a hidden dimension of H, and perform classification over C classes.\n",
        "\n",
        "    The architecture should be fc - relu - fc - softmax with one hidden layer\n",
        "\n",
        "    The learnable parameters of the model are stored in the dictionary\n",
        "    self.params that maps parameter names to numpy arrays.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim=3072, hidden_dim=300, num_classes=10,\n",
        "                 weight_scale=1e-3):\n",
        "        \"\"\"\n",
        "        Initialize a new network.\n",
        "\n",
        "        Inputs:\n",
        "        - input_dim: An integer giving the size of the input\n",
        "        - hidden_dim: An integer giving the size of the hidden layer, None\n",
        "          if there's no hidden layer.\n",
        "        - num_classes: An integer giving the number of classes to classify\n",
        "        - weight_scale: Scalar giving the standard deviation for random\n",
        "          initialization of the weights.\n",
        "        \"\"\"\n",
        "        self.params = {}\n",
        "        ############################################################################\n",
        "        # TODO: Initialize the weights and biases of the two-layer net. Weights    #\n",
        "        # should be initialized from a Gaussian centered at 0.0 with               #\n",
        "        # standard deviation equal to weight_scale, and biases should be           #\n",
        "        # initialized to zero. All weights and biases should be stored in the      #\n",
        "        # dictionary self.params, with fc weights and biases using the keys        #\n",
        "        # 'W' and 'b', i.e., W1, b1 for the weights and bias in the first linear   #\n",
        "        # layer, W2, b2 for the weights and bias in the second linear layer.       #\n",
        "        ############################################################################\n",
        "        #self.hidden_dim = \n",
        "        self.params['W1'] = np.random.randn(input_dim, hidden_dim) * weight_scale \n",
        "        self.params['b1'] = np.zeros(hidden_dim)\n",
        "        self.params['W2'] = np.random.randn(hidden_dim, num_classes) * weight_scale\n",
        "        self.params['b2'] = np.zeros(num_classes)\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "\n",
        "\n",
        "    def forwards_backwards(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Compute loss and gradient for a minibatch of data.\n",
        "\n",
        "        Inputs:\n",
        "        - X: Array of input data of shape (N, Din)\n",
        "        - y: Array of labels, of shape (N,). y[i] gives the label for X[i].\n",
        "\n",
        "        Returns:\n",
        "        If y is None, then run a test-time forward pass of the model and return:\n",
        "        - scores: Array of shape (N, C) giving classification scores, where\n",
        "          scores[i, c] is the classification score for X[i] and class c.\n",
        "\n",
        "        If y is not None, then run a training-time forward and backward pass. And\n",
        "        return a tuple of:\n",
        "        - loss: Scalar value giving the loss\n",
        "        - grads: Dictionary with the same keys as self.params, mapping parameter\n",
        "          names to gradients of the loss with respect to those parameters.\n",
        "        \"\"\"\n",
        "        scores = None\n",
        "        ############################################################################\n",
        "        # TODO: Implement the forward pass for the two-layer net, computing the    #\n",
        "        # class scores for X and storing them in the scores variable.              #\n",
        "        ############################################################################\n",
        "        forward_layer_output_1, forward_layer_cache_1 = fc_forward(X, self.params['W1'], self.params['b1'])\n",
        "        relu_output, relu_cache = relu_forward(forward_layer_output_1)\n",
        "        forward_layer_output_2, forward_layer_cache_2 = fc_forward(relu_output, self.params['W2'], self.params['b2'])\n",
        "        scores = forward_layer_output_2\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "\n",
        "        # If y is None then we are in test mode so just return scores\n",
        "        if y is None:\n",
        "            return scores\n",
        "\n",
        "        loss, grads = 0, {}\n",
        "        ############################################################################\n",
        "        # TODO: Implement the backward pass for the two-layer net. Store the loss  #\n",
        "        # in the loss variable and gradients in the grads dictionary. Compute data #\n",
        "        # loss using softmax, and make sure that grads[k] holds the gradients for  #\n",
        "        # self.params[k].                                                          # \n",
        "        ############################################################################\n",
        "        loss, gradients = softmax_loss(scores, y)\n",
        "        backward_layer_output_2, grads['W2'], grads['b2'] = fc_backward(gradients, forward_layer_cache_2)\n",
        "        backward_layer_relu = relu_backward(backward_layer_output_2, relu_cache)\n",
        "        backward_layer_ouput_1, grads['W1'], grads['b1'] = fc_backward(backward_layer_relu, forward_layer_cache_1)\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "        return loss, grads\n",
        "\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwp0waIL1h_e",
        "colab_type": "text"
      },
      "source": [
        "# 4.2(c) Training\n",
        "\n",
        "In this problem, you need to preprocess the images and set up model hyperparameters. Notice that adjust the training and val split is optional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZPtQzXGMoCg",
        "colab_type": "code",
        "outputId": "70381733-2a1f-46a1-db15-62b854503a75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        }
      },
      "source": [
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding=\"latin1\")\n",
        "    return dict\n",
        "\n",
        "def load_cifar10():\n",
        "    data = {}\n",
        "    meta = unpickle(\"cifar-10-batches-py/batches.meta\")\n",
        "    batch1 = unpickle(\"cifar-10-batches-py/data_batch_1\")\n",
        "    batch2 = unpickle(\"cifar-10-batches-py/data_batch_2\")\n",
        "    batch3 = unpickle(\"cifar-10-batches-py/data_batch_3\")\n",
        "    batch4 = unpickle(\"cifar-10-batches-py/data_batch_4\")\n",
        "    batch5 = unpickle(\"cifar-10-batches-py/data_batch_5\")\n",
        "    test_batch = unpickle(\"cifar-10-batches-py/test_batch\")\n",
        "    X_train = np.vstack((batch1['data'], batch2['data'], batch3['data'],\\\n",
        "                         batch4['data'], batch5['data']))\n",
        "    Y_train = np.array(batch1['labels'] + batch2['labels'] + batch3['labels'] + \n",
        "                       batch4['labels'] + batch5['labels'])\n",
        "    X_test = test_batch['data']\n",
        "    Y_test = test_batch['labels']\n",
        "    \n",
        "    #Preprocess images here                                     \n",
        "    X_train = (X_train-np.mean(X_train,axis=1,keepdims=True))/np.std(X_train,axis=1,keepdims=True)\n",
        "    X_test = (X_test-np.mean(X_test,axis=1,keepdims=True))/np.std(X_test,axis=1,keepdims=True)\n",
        "\n",
        "    data['X_train'] = X_train[:40000]\n",
        "    data['y_train'] = Y_train[:40000]\n",
        "    data['X_val'] = X_train[40000:]\n",
        "    data['y_val'] = Y_train[40000:]\n",
        "    data['X_test'] = X_test\n",
        "    data['y_test'] = Y_test\n",
        "    return data\n",
        "\n",
        "def test_network(model, X, y, num_samples=None, batch_size=100):\n",
        "    \"\"\"\n",
        "    Check accuracy of the model on the provided data.\n",
        "\n",
        "    Inputs:\n",
        "    - model: Image classifier\n",
        "    - X: Array of data, of shape (N, d_1, ..., d_k)\n",
        "    - y: Array of labels, of shape (N,)\n",
        "    - num_samples: If not None, subsample the data and only test the model\n",
        "      on num_samples datapoints.\n",
        "    - batch_size: Split X and y into batches of this size to avoid using\n",
        "      too much memory.\n",
        "\n",
        "    Returns:\n",
        "    - acc: Scalar giving the fraction of instances that were correctly\n",
        "      classified by the model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Subsample the data\n",
        "    N = X.shape[0]\n",
        "    if num_samples is not None and N > num_samples:\n",
        "        mask = np.random.choice(N, num_samples)\n",
        "        N = num_samples\n",
        "        X = X[mask]\n",
        "        y = y[mask]\n",
        "\n",
        "    # Compute predictions in batches\n",
        "    num_batches = N // batch_size\n",
        "    if N % batch_size != 0:\n",
        "        num_batches += 1\n",
        "    y_pred = []\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = (i + 1) * batch_size\n",
        "        scores = model.forwards_backwards(X[start:end])\n",
        "        y_pred.append(np.argmax(scores, axis=1))\n",
        "    y_pred = np.hstack(y_pred)\n",
        "    acc = np.mean(y_pred == y)\n",
        "\n",
        "    return acc\n",
        "\n",
        "\n",
        "def train_network(model, data, **kwargs):\n",
        "    \"\"\"\n",
        "     Required arguments:\n",
        "    - model: Image classifier\n",
        "    - data: A dictionary of training and validation data containing:\n",
        "      'X_train': Array, shape (N_train, d_1, ..., d_k) of training images\n",
        "      'X_val': Array, shape (N_val, d_1, ..., d_k) of validation images\n",
        "      'y_train': Array, shape (N_train,) of labels for training images\n",
        "      'y_val': Array, shape (N_val,) of labels for validation images\n",
        "\n",
        "    Optional arguments:\n",
        "    - learning_rate: A scalar for initial learning rate.\n",
        "    - lr_decay: A scalar for learning rate decay; after each epoch the\n",
        "      learning rate is multiplied by this value.\n",
        "    - batch_size: Size of minibatches used to compute loss and gradient\n",
        "      during training.\n",
        "    - num_epochs: The number of epochs to run for during training.\n",
        "    - print_every: Integer; training losses will be printed every\n",
        "      print_every iterations.\n",
        "    - verbose: Boolean; if set to false then no output will be printed\n",
        "      during training.\n",
        "    - num_train_samples: Number of training samples used to check training\n",
        "      accuracy; default is 1000; set to None to use entire training set.\n",
        "    - num_val_samples: Number of validation samples to use to check val\n",
        "      accuracy; default is None, which uses the entire validation set.\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    learning_rate =  kwargs.pop('learning_rate', 1e-3)\n",
        "    lr_decay = kwargs.pop('lr_decay', 1.0)\n",
        "    batch_size = kwargs.pop('batch_size', 100)\n",
        "    num_epochs = kwargs.pop('num_epochs', 10)\n",
        "    num_train_samples = kwargs.pop('num_train_samples', 1000)\n",
        "    num_val_samples = kwargs.pop('num_val_samples', None)\n",
        "    print_every = kwargs.pop('print_every', 10)   \n",
        "    verbose = kwargs.pop('verbose', True)\n",
        "    \n",
        "    epoch = 0\n",
        "    best_val_acc = 0\n",
        "    best_params = {}\n",
        "    loss_history = []\n",
        "    train_acc_history = []\n",
        "    val_acc_history = []\n",
        "    \n",
        "    \n",
        "    num_train = data['X_train'].shape[0]\n",
        "    iterations_per_epoch = max(num_train // batch_size, 1)\n",
        "    num_iterations = num_epochs * iterations_per_epoch\n",
        "    \n",
        "\n",
        "    \n",
        "    for t in range(num_iterations):\n",
        "        # Make a minibatch of training data\n",
        "        batch_mask = np.random.choice(num_train, batch_size)\n",
        "        X_batch = data['X_train'][batch_mask]\n",
        "        y_batch = data['y_train'][batch_mask]\n",
        "        \n",
        "        # Compute loss and gradient\n",
        "        loss, grads = model.forwards_backwards(X_batch, y_batch)\n",
        "        loss_history.append(loss)\n",
        "\n",
        "        # Perform a parameter update\n",
        "        for p, w in model.params.items():\n",
        "            model.params[p] = w - grads[p]*learning_rate\n",
        "          \n",
        "        # Print training loss\n",
        "        if verbose and t % print_every == 0:\n",
        "            print('(Iteration %d / %d) loss: %f' % (\n",
        "                   t + 1, num_iterations, loss_history[-1]))\n",
        "         \n",
        "        # At the end of every epoch, increment the epoch counter and decay\n",
        "        # the learning rate.\n",
        "        epoch_end = (t + 1) % iterations_per_epoch == 0\n",
        "        if epoch_end:\n",
        "            epoch += 1\n",
        "            learning_rate *= lr_decay\n",
        "        \n",
        "        # Check train and val accuracy on the first iteration, the last\n",
        "        # iteration, and at the end of each epoch.\n",
        "        first_it = (t == 0)\n",
        "        last_it = (t == num_iterations - 1)\n",
        "        if first_it or last_it or epoch_end:\n",
        "            train_acc = test_network(model, data['X_train'], data['y_train'],\n",
        "                num_samples= num_train_samples)\n",
        "            val_acc = test_network(model, data['X_val'], data['y_val'],\n",
        "                num_samples=num_val_samples)\n",
        "            train_acc_history.append(train_acc)\n",
        "            val_acc_history.append(val_acc)\n",
        "\n",
        "            if verbose:\n",
        "                print('(Epoch %d / %d) train acc: %f; val_acc: %f' % (\n",
        "                       epoch, num_epochs, train_acc, val_acc))\n",
        "\n",
        "            # Keep track of the best model\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                best_params = {}\n",
        "                for k, v in model.params.items():\n",
        "                    best_params[k] = v.copy()\n",
        "        \n",
        "    model.params = best_params\n",
        "        \n",
        "    return model, train_acc_history, val_acc_history\n",
        "        \n",
        "\n",
        "# load data\n",
        "data = load_cifar10() \n",
        "train_data = { k: data[k] for k in ['X_train', 'y_train', \n",
        "                                    'X_val', 'y_val']}\n",
        "#######################################################################\n",
        "# TODO: Set up model hyperparameters                                  #\n",
        "#######################################################################\n",
        "\n",
        "# initialize model\n",
        "model = SoftmaxClassifier(hidden_dim =350, weight_scale=1e-2)\n",
        "\n",
        "# start training    \n",
        "model, train_acc_history, val_acc_history = train_network(\n",
        "    model, train_data, learning_rate =1e-3 ,\n",
        "    lr_decay=1.0, num_epochs=10, \n",
        "    batch_size=10, print_every=1000)\n",
        "#######################################################################\n",
        "#                         END OF YOUR CODE                            #\n",
        "#######################################################################\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(Iteration 1 / 40000) loss: 2.333097\n",
            "(Epoch 0 / 10) train acc: 0.104000; val_acc: 0.107400\n",
            "(Iteration 1001 / 40000) loss: 2.190750\n",
            "(Iteration 2001 / 40000) loss: 2.070655\n",
            "(Iteration 3001 / 40000) loss: 1.900290\n",
            "(Epoch 1 / 10) train acc: 0.396000; val_acc: 0.365200\n",
            "(Iteration 4001 / 40000) loss: 2.324320\n",
            "(Iteration 5001 / 40000) loss: 1.795993\n",
            "(Iteration 6001 / 40000) loss: 1.531608\n",
            "(Iteration 7001 / 40000) loss: 1.605323\n",
            "(Epoch 2 / 10) train acc: 0.423000; val_acc: 0.412400\n",
            "(Iteration 8001 / 40000) loss: 1.427910\n",
            "(Iteration 9001 / 40000) loss: 1.654643\n",
            "(Iteration 10001 / 40000) loss: 1.476426\n",
            "(Iteration 11001 / 40000) loss: 1.477906\n",
            "(Epoch 3 / 10) train acc: 0.460000; val_acc: 0.429600\n",
            "(Iteration 12001 / 40000) loss: 1.492595\n",
            "(Iteration 13001 / 40000) loss: 1.601732\n",
            "(Iteration 14001 / 40000) loss: 1.370751\n",
            "(Iteration 15001 / 40000) loss: 1.381645\n",
            "(Epoch 4 / 10) train acc: 0.461000; val_acc: 0.448000\n",
            "(Iteration 16001 / 40000) loss: 1.270154\n",
            "(Iteration 17001 / 40000) loss: 1.024541\n",
            "(Iteration 18001 / 40000) loss: 1.028031\n",
            "(Iteration 19001 / 40000) loss: 1.652557\n",
            "(Epoch 5 / 10) train acc: 0.508000; val_acc: 0.463600\n",
            "(Iteration 20001 / 40000) loss: 1.787535\n",
            "(Iteration 21001 / 40000) loss: 1.837165\n",
            "(Iteration 22001 / 40000) loss: 2.073164\n",
            "(Iteration 23001 / 40000) loss: 1.847389\n",
            "(Epoch 6 / 10) train acc: 0.547000; val_acc: 0.470500\n",
            "(Iteration 24001 / 40000) loss: 1.490032\n",
            "(Iteration 25001 / 40000) loss: 1.258671\n",
            "(Iteration 26001 / 40000) loss: 1.516922\n",
            "(Iteration 27001 / 40000) loss: 1.701207\n",
            "(Epoch 7 / 10) train acc: 0.511000; val_acc: 0.480800\n",
            "(Iteration 28001 / 40000) loss: 1.813431\n",
            "(Iteration 29001 / 40000) loss: 0.820033\n",
            "(Iteration 30001 / 40000) loss: 1.462034\n",
            "(Iteration 31001 / 40000) loss: 2.364825\n",
            "(Epoch 8 / 10) train acc: 0.570000; val_acc: 0.487000\n",
            "(Iteration 32001 / 40000) loss: 1.879465\n",
            "(Iteration 33001 / 40000) loss: 1.371678\n",
            "(Iteration 34001 / 40000) loss: 0.980498\n",
            "(Iteration 35001 / 40000) loss: 1.576502\n",
            "(Epoch 9 / 10) train acc: 0.520000; val_acc: 0.494000\n",
            "(Iteration 36001 / 40000) loss: 1.535015\n",
            "(Iteration 37001 / 40000) loss: 1.020249\n",
            "(Iteration 38001 / 40000) loss: 1.073417\n",
            "(Iteration 39001 / 40000) loss: 1.056425\n",
            "(Epoch 10 / 10) train acc: 0.583000; val_acc: 0.502800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcovGmpXvXXa",
        "colab_type": "text"
      },
      "source": [
        "# 4.2(c) Report Accuracy\n",
        "\n",
        "Run the given code and report the accuracy on test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwCq8pBhu6dz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a4dbeda7-de67-4dc4-de20-b023f5573f81"
      },
      "source": [
        "# report test accuracy\n",
        "acc = test_network(model, data['X_test'], data['y_test'])\n",
        "print(\"Test accuracy: {}\".format(acc))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.5003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTrmbULS7i2N",
        "colab_type": "text"
      },
      "source": [
        "# 4.2(d) Plot\n",
        "\n",
        "Using the train_acc_history and val_acc_history, plot the train & val accuracy versus epochs on one plot. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPjtnbya9S7g",
        "colab_type": "code",
        "outputId": "ad12781b-b9f4-4ac7-8050-e0007e586452",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "num_epochs=range(0,11)\n",
        "plt.plot(num_epochs, train_acc_history, 'r', label='Training accuracy')\n",
        "plt.plot(num_epochs, val_acc_history, 'b', label='Validation accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Us5JiHIoMlW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}